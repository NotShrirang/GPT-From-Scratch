{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wkh9Gy6AVo_",
        "outputId": "7747db02-be21-4154-a7d6-33ea0b12148b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j701WcsRp8tz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClV9HF5ep-lO"
      },
      "outputs": [],
      "source": [
        "batch_size = 8 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 12\n",
        "n_layer = 18\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "155BIh3eqCWJ",
        "outputId": "c1ae8b37-b7d8-4119-9455-bed0c885365f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d556ff95a10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s0ZsjvGqEzC",
        "outputId": "10a80990-1370-472b-ab43-6e53d328e8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-22 01:05:43--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-03-22 01:05:44 (23.0 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntfw4MmcqIxA"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaTZKvLWqLSW"
      },
      "outputs": [],
      "source": [
        "words = text.split()\n",
        "vocab_size = len(words)\n",
        "stoi = {word: i for i, word in enumerate(words)}\n",
        "itos = {i: word for i, word in enumerate(words)}\n",
        "\n",
        "def encode(s): return [stoi[w] for w in s.split()]\n",
        "\n",
        "def decode(ids): return ' '.join([itos[i] for i in ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7xZcajy1qaF"
      },
      "outputs": [],
      "source": [
        "# chars = sorted(list(set(text)))\n",
        "# vocab_size = len(chars)\n",
        "# # create a mapping from characters to integers\n",
        "# stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "# itos = {i: ch for i, ch in enumerate(chars)}\n",
        "# # encoder: take a string, output a list of integers\n",
        "# def encode(s): return [stoi[c] for c in s]\n",
        "# # decoder: take a list of integers, output a string\n",
        "# def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "# # Train and test splits\n",
        "# data = torch.tensor(encode(text), dtype=torch.long)\n",
        "# n = int(0.9*len(data))  # first 90% will be train, rest val\n",
        "# train_data = data[:n]\n",
        "# val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYdIpTfjqNpt"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lW6UAL2qQx8"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjPGf14wqSOK"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_CsH_DApyMw"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(\n",
        "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(\n",
        "            torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZrNDmhIqfRZ",
        "outputId": "597c3ef0-838c-41fb-c8e1-a9100aac4f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "187.857307 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvhBnRXwqbZy",
        "outputId": "36a7cccf-0913-47e6-cd0a-f7b9714fbec6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5000 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 12.2774, val loss 12.2715\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 500/5000 [06:50<50:42,  1.48it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 500: train loss 6.4428, val loss 7.8523\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1000/5000 [13:41<44:25,  1.50it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1000: train loss 5.5890, val loss 7.9600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 1500/5000 [20:33<38:58,  1.50it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1500: train loss 4.8308, val loss 8.2451\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2000/5000 [27:24<33:15,  1.50it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2000: train loss 3.9608, val loss 8.6823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2500/5000 [34:16<28:00,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2500: train loss 2.7539, val loss 9.4397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 3000/5000 [41:07<22:43,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3000: train loss 1.6118, val loss 10.2757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 3500/5000 [47:58<16:38,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3500: train loss 0.8286, val loss 11.2022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 4000/5000 [54:49<11:04,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 4000: train loss 0.4558, val loss 11.9278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 4500/5000 [1:01:41<05:35,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 4500: train loss 0.3210, val loss 12.4486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 4999/5000 [1:08:33<00:00,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 4999: train loss 0.2558, val loss 12.8638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [1:09:48<00:00,  1.19it/s]\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in tqdm.tqdm(range(max_iters)):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(\n",
        "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3rCPe1_qpu2",
        "outputId": "40d11251-dadd-440a-ed74-5f498443045e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First these with the Tower, For every thing dispatch; you, sir, I saw When you are better husband. Consenting born: the heaven will take your company, my soldier, statesman, all: it stands your voices she earthly, no displeasure's peril with his eyes He made you came near your city; She may Enjoy your justice do With his father, As to give him fair, Might call the women are your wives, As much honour, I will Standing your talk; so trouble fair and prosperity! What 'twere fair, Which, as those that loves me; as many have forgot my lord, as she is, Who is, as he grows good comfort, On all acknowledge. For Polixenes, And royalties of my up as I prithee, give course of Warwick!' sees arm burns myself. MAMILLIUS: Nay, lady ne'er weep. CAMILLO: Hold me, as e'er I may question My nose that my father too; Or meaner father, now I am arrived At home, and, with other while we call me we are. LEONTES: Then do so, I am put apart these ceremony. MEASURE. Then, master is it, If thou wouldst be strange. He's damned witchcraft, and of my mind to behold a want of Crosby Place. But, sirs, He shall sear me awhile to the issue. HERMIONE: The stony weakness. If Should prove of this: my lord, Be thou draw the issue. PAULINA: It is Attend me withdraw it? only thyself, And sends No more bodies Which quired with my lord. First Murderer: Tush! Fear to my lord. First Murderer: Nor thou it up. Second Murderer: Go, go, my lord. First Murderer: We hear The noble brave lords, we all determine them again, And through the queen's three suspicion. Servant: Take my lord, we'll that same ancient deceivest thyself: 'Tis so much sing, would have deserved of you; never more soon there with those who was the rest, whom we fear sad councils, I. First lord, we nothing less for ourselves: you did suffer this? BRAKENBURY: Your mind, I fear to greet 'em. We may in force will then to tell God's dreadful have thought of wounds for this, and your hands. PRINCE EDWARD: Lord! Rivers and Hastings, hope: 'Tis no lord of master guess, But 'twas too much. RICHARD: Alack, why else the gates of you hither? Messenger: The news abroad? First Murderer: I are humble. Second Murderer: To, to, to-- CLARENCE: His wit the lords help all the lords and relieve him, let us lean to tell thee but still the queen at hand: intend to make true urged to London with conscience and Hastings: let us ha' it, Until your hands: Who affrighted you; As I met them; each advantage that fight with you see what reason, pray you, fly to put back from the meaning. Here comes your lordship. HASTINGS: Come, citizens: 'zounds! I'll give nothing have not see that now crave them play the hearts Notice to save this night. I hope my lord? HASTINGS: What, have thought, why not stand from Buckingham. CATESBY: You promised me but that you may. A holy cousin, my lord, seven years to see what I hope the effect what they have said, As little mean the land that's on! But, stay: what I need my lord. GLOUCESTER: Well have forgot your boar-spear, man? Richard; that I hope I would show your lordship now. HASTINGS: I saw us along; and that's buzzards prey of what 'twere well. The prince of frozen ridges of their Namely, to you. Lord Aumerle. It is dead. GLOUCESTER: What, art sworn marry, tell that I am doth it is true. DUKE OF YORK: Well, then, you must awake, and therefore rise and others must we will be hang'd. GLOUCESTER: Come you shall lose your grace: grow to summon it. My lord, I'll take your hand; Nor importune unto your love. DUKE OF YORK: I'll salute you so, I toward that you so? Little are sent to my place: Because then, away; he knows, down. Make haste; them. KING RICHARD II: Twice think You said uncle, come into ourselves to be gone too. KING RICHARD II: What is a witness to make lent him what think it twenty is seldom ten thousand sea? league: I am accused, And these sad dog with no blood since it were struck bad dealings must be, by presence may not the king for him all; what children from some moe it were it brings; years hath deadly was virtue called upon his tongue. Things known to do so, it is the offer of treachery of force must say; that is meet me. When you that is our state that sorrow that justice seem'st thou didst not have it so! Do that any further to subjects, corn, When with me. and our king: For though authority himself! how near we mean to break in Rome. Second Keeper: We have sworn in nature: for our bands: Let us be it is spoke found one rose Against the winter is. Music do ha' first: why not I build of men's faces, But if they are to save both change and perish. Bad news, and another man's care be past doubt, To serve what they have made you think they are to composition with me. Some remedies so short in thinking on the extremity of it must needs must needs must be violent his back? Third love. Come, lords, I dare home to lies Against the heart what absence of Gloucester's death. Second Murderer: O royal master's majesty was the majesty hath made us to put forth to foot, Doth it so? then he hear his meed, go back: do you to him. GLOUCESTER: Come, cousin, that gracious lord? KING RICHARD III: He makes your Edward fiery lie tonight; commanded back? Where are the giving him that once looks That they are the giving him high war; Nor in this sepulchre? What place for I have braved the marriage with him? Some certain his know? CLARENCE: Send me, but, if\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Write a scene about Romeo arguing with Juliet.\n",
        "ROMEO:\"\"\"\n",
        "input = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
        "print(decode(m.generate(input, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "qZFV2aVZmOqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd81ac3-740c-44f4-8317-3603849b6ab9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a scene about Romeo arguing with Juliet. ROMEO: Tybalt gone, Romeo! Who thought it is not a kinsman will teach marriage she lies, And in love worn her laid the deed. JULIET: I pray for thou desire my lady from me to thee at leisure, holy friar, That love here I would I tell you me now; Or shall I have no more ease. ROMEO: I would be satisfied With what wrong I am too much to the pedlar toward the year, By circumstance, but time till weeping ill, soon disperse that night. JULIET: No, coz, I tell thee dead! O, tell thee remedy. JULIET: I know pleased then. I wish thee word more: For whom, my lady's talk'd of Paris, I will. JULIET: But stay, My people do remember but a wanton's bird; Who lets it hop a poor prisoner in his twisted gyves, And with a silk Out on mine own. QUEEN MARGARET: Am never be not with a silk thread plucks it back again, So loving-jealous of his twisted gyves, And with much cherishing. Good night. PRINCE: Is such sweet prince. What day to his way? Is such sweet Clarence, till that hand, and twenty men, good world have bless'd thee with a careful watch, churchyard tread, Being vex'd a meeting of wine. Rest this? ROMEO: It straight shall I smell the shadow and her prodigy, Being this! FRIAR LAURENCE: At the breath that helps weep the courageous captain of day! Never have found lies sum of their client woes, would ignoble forerun my back, Shall be satisfied, dear hap is slain, and fiery wheels: The breath of fiery wheels: Now, ere the lark, that kiss your burning Saint George fall into England's steps To season after your course, If that hast got I must but yet it sleep never to Coventry, upon his mercy, to ope the cursed either eye: Why, what my heart and night's dank dew to woe, I must enter owes; but grief they have prevail'd but my fair rose With nothing hereafter keep that I see, At tell. FRIAR LAURENCE: The obsequies that tells my ears have I wander from arm of light, But when is the house were on roan Barbary, That horse that blows but love's rite? What say that blows the fearful hollow ground; Where thou hast thou hast done! Servant: God knows thou shalt adventure to me: for that word's death; I may not: So shall well for me; And that thou, fought, And that would have returned dear! So shall be so happy days mistrust not, in the other. BALTHASAR: I am in the death-darting woes are pale reflex of death of Lancaster? Where is not when 'tis just, my mind! lie. Servant: My lord, there were convey me to despair. This child that there were gone? O, but trusting France: After the way being gentlemen, Be short dead, sir; He hath the duke is as long is't goes with When his hopes there. Servant: He is come. JULIET: That which is luck, being gentlemen, what of you, Lest that I mean for both: hath the loss of late of love? ROMEO: Ay me! Was it is true. is going to know, as thou hast slander'd it. It is alter'd: that I should soonest might with you now. My lord, I have so; we shall indeed; and now to look aside the proofs. The earth from whence I count this see our kinsman, noble soul, And we must swear to our daughter: I pray thee, provided A horse hath true. STANLEY: It is not me, that is too holy cousin, I hope, thou know'st, is impossible. RICHARD: Ah, gentle Richard, he hath not his man, And made encounter me: ere to slaughter thee. YORK: An hour it that I so offended, He hath dead; and yet, my lord, for anger burns; I would break some little word! Farewell, my lord, And he's ta'en, Therefore let him sit before the princes straight: Wast could move our lights choose they weep our brows are ready for their graves Whilst I, when I saw the ground: attorneys are fast like success? 3 KING HENRY VI RUTLAND: Ah, whither could tell us tortures Nor men to France for brave Northumberland. NORTHUMBERLAND: untrodden stones? And what hear that pardon me. die so short in woe. RUTLAND: Then let us all alone will not smile in this day, my lord, take it not sit again? RICHARD: Why, how it makes Southam Ay, this molehill will for us! CLIFFORD: Come, cousin, sit your honour dares it. CLIFFORD: Not death, nor on the queen hath lives thee mercy, Lancaster, study for all our cousin, do lament, And courage makes in those that led your honour could not stay. HENRY hour, Your followers to trouble there's charity. CLIFFORD: It toucheth us all. HENRY VI: Exeter, did water dying men may the assault? There, are all hath got his hand were his good old suits got their limits stain'd the axe before the field And no think, as heaven keeps his Antiates; And that Warwick off the water Even that love it. It is yours. EXTON: Because that were of question,--that you, having the crown your former time; blows for that once Than when I would unseen of the king your rude hand prompts you, great bounty, that false subjects having press'd course, who, heavens be found: Now speaking it, Or ten times new world's crown, For who over, For who hath ever did sleep then! EDWARD: O, that it still from France Ere now looks at mountains with age to a woman here some man did shrink from the shadow with him? When renown press'd before his breast through the battle's ended, If hand by his breast Left fools should be immortal proud err'd: Then shall make you his ruthless waves, with speed; Or courage had any creeping forget them gaze; I heard some sign of mine, blows than fill your words. Unreasonable creatures another. NORTHUMBERLAND: My lord, even with grief, And nothing but\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oooh we see some improvement!"
      ],
      "metadata": {
        "id": "z5bBb8S-nG90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nfeS4V4g03Jl"
      },
      "outputs": [],
      "source": [
        "torch.save(m.state_dict(), '/content/Trained Model/GPT_model_word.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Bsmr9Hco1yC8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "src=\"/content/Trained Model\"\n",
        "des=\"/content/drive/MyDrive/GPT From Scratch/Word\"\n",
        "if os.path.exists(des):\n",
        "        shutil.rmtree(des)\n",
        "        shutil.copytree(src, des)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjYC-AA9BPEV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}